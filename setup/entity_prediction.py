# -*- coding: utf-8 -*-
"""Entity prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lc1_ox-LJl0iqiQ96kmFO1UOt4K627RS
"""

# making all essential imports
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
import numpy as np
import pandas as pd
import re
import tensorflow as tf
import torch
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM, Input
from keras.layers import Embedding
from keras.preprocessing.text import Tokenizer

import warnings
warnings.filterwarnings("ignore")

#load datasets from ACLED (note: you will need to create an ACLED account and generate an access key)
data1 = pd.read_csv(r'/content/ACLED Nigeria 2019-2020.csv')
data2 = pd.read_csv(r'/content/ACLED Nigeria 2019-2020.csv')
df = pd.concat([data1, data2], axis=0)

#datafram with notes and unique IDs
df = df[['event_id_no_cnty','notes']]
df.event_id_no_cnty = df.event_id_no_cnty.astype(str)
df = df.reset_index(drop = True)
df

# create a named entity recognition function
def notes_to_entities(notes):
    words = nltk.word_tokenize(notes)
    tag = nltk.pos_tag(words)
    chunks = nltk.ne_chunk(tag, binary=False)
    entities =[]
    labels =[]
    for chunk in chunks:
        if hasattr(chunk,'label'):
            entities.append(' '.join(c[0] for c in chunk))
            labels.append(chunk.label())
        
    entities = ','.join(entities)
    return entities

# convert notes column to a list of entities
ent_list = []
for note in df.notes:
    temp = notes_to_entities(note)
    ent_list.append(temp)
df['entities'] = ent_list

df['entities']

# pre process entities column
df['entities'] = df.entities.apply(lambda x: x[0:-1].split(','))

# convert entities column to a list of lists
ent_list = df['entities']
entities = []
for i in range(len(ent_list)):
  entities.append(ent_list[i]) 
entities

# using gensim import word2vec and vectorize the entities list
from gensim.models import Word2Vec
word2vec = Word2Vec(entities, min_count=1)
word2vec

# saving all the vocabulary used in word2vec
vocabulary = list(word2vec.wv.vocab)

# create a text sequence of text 3 to predict 3rd entity based on the first 2
train_len = 3
text_sequences = []
for i in range(train_len,len(vocabulary)):
  seq = vocabulary[i-train_len:i]
  text_sequences.append(seq)
text_sequences

# seperating training sequences and labels. The first 2 entities are being used to predict the third. Think of this as X_train and Y_train

train_sequence = [[word2vec[ingredient] for ingredient in sequence[:2]] for sequence in text_sequences]
train_sequence

label_sequence = [[word2vec[ingredient] for ingredient in sequence[2:]] for sequence in text_sequences]
label_sequence

np.array(train_sequence).shape

np.array(label_sequence).shape

# converting lists to numpy array for feeding to below neural net
train_inputs = np.array(train_sequence)
train_targets = np.array(label_sequence)
train_targets = train_targets.reshape(train_targets.shape[0],train_targets.shape[2])
train_targets.shape, train_inputs.shape

# setting up the neural net with 2 LSTM and 2 Dense Layers
model = Sequential()
model.add(Input(shape=(2,100)))
model.add(LSTM(100,return_sequences=True))
model.add(LSTM(50))
model.add(Dense(50,activation='relu'))
model.add(Dense(100, activation='relu'))

opt = tf.keras.optimizers.Adam(
    learning_rate=0.001,
    name="Adam"
)
model.compile(loss='mse', optimizer=opt, metrics=['accuracy'])

model.summary()

# model training on inputs 
history = model.fit(train_inputs,train_targets,epochs=100, validation_split=0.15)

def get_linked_entities(input_news):
    text = notes_to_entities(input_news)
    input_text = text[:].split(',')
    input_text = input_text[0:2]

    # converts text to vector
    input_vector = [word2vec[idx] if idx in word2vec else np.zeros((100,)) for idx in input_text]
    if len(input_vector)!=2:
      while len(input_vector)!=2:
        input_vector.append(np.zeros((100,)))

    # converts input vector to numpy array
    input_vector = np.array([input_vector])

    # # getting model predictions for top 10 suggestions by finding the 10 most similar ingredients to our output
    output_vector = model.predict(input_vector)
    pred = word2vec.most_similar(positive=[output_vector.reshape(100,)], topn=5)
    print("Top 5 linked entities:")
    return pred

get_linked_entities('On 31 December 2019, Nigerian Air Force conducted air strikes against a camp of Boko Haram (JAS) in Abulam (Damboa, Borno). Equipment was destroyed and an unreported number of BH fighters killed. Unknown number of fatalities coded as 10.')
